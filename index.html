
<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>A Gentle Introduction to Deep Learning with Tensorflow - PyCon 2017</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/beige.css">
		<link rel="stylesheet" href="css/custom.css">


		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/googlecode.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<!-- Title slide -->
				<section>
					<h2>A gentle introduction to deep learning with TensorFlow</h2>
					<p>Michelle Fullwood<br />
					@michelleful</p>

					<p>&nbsp;</p>
					<p>Slides: michelleful.github.io/PyCon2017</p>
				</section>

<!-- Introduction -->
		 <section>

				<section>
					<h2>Target</h2>
          (Deep) Feed-forward neural networks
					<p>
					<img src="images/nnz_mlp.png"
					     style="width: 20%;">
					</p>
					<ul>
						<li>How they're constructed</li>
						<li>Why they work</li>
						<li>How to train and optimize them</li>
					</ul>
					<p style="font-size: 35%; text-align: left;">Image source: Fjodor van Veen (2016) <a href="http://www.asimovinstitute.org/neural-network-zoo/">Neural Network Zoo</a></p>

				</section>

				<section data-transition="fade-out">
					<h2>Deep learning learning curve</h2>
					<img src="images/latex_generated_images/learning_curve_nopoints.png"
					     alt="Completely made-up learning curve"
							 style="width: 60%;">
				</section>

				<section data-transition="fade">
					<h2>Deep learning learning curve</h2>
					<img src="images/latex_generated_images/learning_curve_target.png"
					     alt="Our goal is here: completely understanding of the most
							      fundamental class of neural networks, feedforward networks
										or multi-layer perceptrons. How they work, how to train them,
										how to optimize them."
							 style="width: 60%;">
				</section>

				<section>
					<h2>Starting point</h2>
					<ul>
						<li>Knowledge of concepts of supervised learning</li>
						<li>Familiarity with linear and logistic regression</li>
					</ul>
				</section>

				<section data-transition="fade">
					<h2>Deep learning learning curve</h2>
					<img src="images/latex_generated_images/learning_curve_not_here.png"
					     alt="If you meet the prerequisites for this talk, namely,
							      you know the fundamentals of supervised machine learning
										and are familiar with linear and logistic regression,
										then you're not here..."
							 style="width: 60%;">
				</section>

				<section data-transition="fade">
					<h2>Deep learning learning curve</h2>
					<img src="images/latex_generated_images/learning_curve_nor_here.png"
					     alt="...nor here..."
							 style="width: 60%;">
				</section>

				<section data-transition="fade">
					<h2>Deep learning learning curve</h2>
					<img src="images/latex_generated_images/learning_curve_but_here.png"
					     alt="but you're actually here!"
							 style="width: 60%;">
				</section>

				<section>
					<table style="border-collapse:collapse;">
						<tr>
							<td style="border: none; text-align: center">
								<img src="images/hammer.png"
								     alt="To use a metaphor, "
									   style="width: 50%; display: block;
										        margin-left: auto; margin-right: auto;">
							</td>
							<td style="border: none; text-align: center;">
								<div class="fragment" data-fragment-index="1">
								<img src="images/tensorflow_logo_big.png"
						         style="width: 55%; display: block;
						                margin-left: auto; margin-right: auto">
								</div>
							</td>
						</tr>
						<tr>
							<td style="text-align: center;">Traditional machine learning</td>
							<td style="text-align: center;"><div class="fragment" data-fragment-index="1">Deep learning</div></td>
						</tr>
					</table>
				</section>

				<section>
					<h2>TensorFlow</h2>
					<ul>
						<li>Popular deep learning toolkit</li>
						<li>From Google Brain, Apache-licensed</li>
						<li>Python API, makes calls to C++ back-end</li>
						<li>Works on CPUs and GPUs</li>
					</ul>
				</section>
			</section>

<!-- LINEAR REGRESSION IN NUMPY -->
			<section>

				<section>
					<h2>Linear Regression<br/>from scratch</h2>
				</section>

				<section>
					<h2>Linear Regression</h2>
					<img src="images/regression_feature_floor_area.png"
						   style="width:14%; margin:2%"
							 alt = "Feature: floor area"
							 class="fragment" data-fragment-index="3">
					<img src="images/regression_feature_distance.png"
							 style="width:18%; margin:2%;"
							 alt="Feature: distance from public transportation"
							 class="fragment" data-fragment-index="4">
					<img src="images/regression_feature_number_of_rooms.png"
							 alt="Feature: number of bedrooms"
					     style="width:14%; margin:2%"
							 class="fragment" data-fragment-index="5">
				  <img src="images/right_arrow.png"
							 alt="are predictors for"
	 					   style="width:15%"
							 class="fragment" data-fragment-index="2">
					<img src="images/regression_target_house_price.png"
						   alt="Target value for regression: house price"
					     style="width:18%;">
				</section>

				<section>
					<h2>Inputs</h2>
					<img src="images/latex_generated_images/matrix_big_numbers.png"
						   style="width:80%"
							 alt = "Represent multiple x's in an mxn matrix
							        and y's in a mx1 vector">
				</section>

				<section>
					<h2>Inputs</h2>
<pre>
<code data-trim data-noescape class="python">
X_train = np.array([
    [1250, 350, 3],
    [1700, 900, 6],
    [1400, 600, 3]
])

Y_train = np.array([345000, 580000, 360000])
</code>
</pre>
				</section>

				<section>
					<h2>Model</h2>
					<p>
						Multiply each <b>feature</b> by a <b>weight</b> and add them up.<br/>
						Add an <b>intercept</b> to get our final <b>estimate</b>.
					</p>
				</section>

				<section>
					<h2>Model</h2>
					<img src="images/latex_generated_images/linear_regression_2d_fitline.png"
						   style="width:50%"
							 alt = "Linear regression is a straight line">
				</section>

				<section>
					<h2>Model - Parameters</h2>
					<pre>
						<code data-trim data-noescape class="python">
weights = np.array([300, -10, -1])
intercept = -26497
						</code>
					</pre>
				</section>

				<section data-transition="fade">
					<h2>Model - Operations</h2>
<img src="images/latex_generated_images/matrix_mult_4.png"
	   style="width:70%"
		 alt = "TODO: show calculation, then show addition of intercept">
				</section>

				<section>
					<h2>Model - Operations</h2>
					<pre>
						<code data-trim data-noescape class="python">
def model(X, weights, intercept):
    return X.dot(weights) + intercept

Y_hat = model(X_train, weights, intercept)
						</code>
					</pre>
				</section>

				<section data-transition="fade-out">
					<h2>Model - Cost function</h2>
					<img src="images/latex_generated_images/linear_regression_2d_multiplepoints.png"
						   style="width:50%"
							 alt = "Okay so we had a pretty bad fit...let's measure it">
				</section>

				<section data-transition="fade">
					<h2>Model - Cost function</h2>
					<img src="images/latex_generated_images/linear_regression_2d_multiplepoints_witherrorlines.png"
						   style="width:50%"
							 alt = "Drop a line from the actual y to the estimated y_hat">
				</section>

				<section data-transition="fade-in">
					<h2>Model - Cost function</h2>
					<img src="images/latex_generated_images/linear_regression_2d_multiplepoints_betterfit.png"
						   style="width:50%"
							 alt = "Drop a line from the actual y to the estimated y_hat">
				</section>

				<section>
					<h2>Cost function</h2>
					<pre>
						<code data-trim data-noescape class="python">
def cost(Y_hat, Y):
    return np.sum((Y_hat - Y)**2)
						</code>
				</section>

				<section>
					<h2>Optimization</h2>
<p>Hold X and Y constant.<br/>Adjust <b>parameters</b> to minimize <b>cost</b>.</p>
				</section>

				<section data-transition="fade-out">
					<h2>Optimization</h2>
<img src="images/latex_generated_images/cost_function_no_tangents.png"
     alt="Graph of cost with respect to weights"
		 style="width: 50%;">
				</section>

				<section>
					<h2>Trial and error</h2>
<img src="images/shooting_hoops.jpg"
     alt="Shooting hoops - adjust angle by trial and error"
		 style="width: 50%;">
<p style="font-size: 35%; text-align: left;">Image source: <a href="https://commons.wikimedia.org/wiki/File:Barack_Obama_playing_basketball.jpg">Wikimedia Commons</a></p>
				</section>

				<section data-transition="fade">
					<h2>Optimization</h2>
<img src="images/latex_generated_images/cost_function_with_tangents.png"
     alt="Follow tangents down to the weight with the lowest cost"
		 style="width: 50%;">
				</section>

				<section data-transition="fade-out">
					<h2>Optimization</h2>
<img src="images/latex_generated_images/cost_function_goldilocks.png"
     alt="Nice trajectory down towards the inimum"
		 style="width: 50%;">
				</section>

				<section data-transition="fade-out">
					<h2>Optimization - Gradient Calculation</h2>
<p>$$\hat{y} = w_0x_0 + w_1x_1 + w_2x_2 + b$$
$$\epsilon = (y-\hat{y})^2$$</p>

<p>&nbsp;</p>

<p><b>Goal:</b> \(\frac{\partial\epsilon}{\partial w_i}, \frac{\partial\epsilon}{\partial b}\)</p>
				</section>

				<section data-transition="fade">
					<h2>Optimization - Gradient Calculation</h2>
<p>\(\frac{\partial\hat{y}}{\partial w_0} = x_0\)</p>
<p>\(\frac{d\epsilon}{d\hat{y}} = -2(y-\hat{y})\)</p>

<p>&nbsp;</p>

<p><b>Chain rule:</b> \(\frac{\partial\epsilon}{\partial w_0} =
	    \frac{d\epsilon}{d\hat{y}}\frac{\partial\hat{y}}{\partial w_0} \)
				</section>

				<section data-transition="fade">
					<h2>Optimization - Gradient Calculation</h2>
<p>$$\hat{y} = w_0x_0 + w_1x_1 + w_2x_2 + b$$</p>

<p>&nbsp;</p>

<p>\(\frac{\partial\hat{y}}{\partial w_0} =\)<span class="fragment">\( x_0\)</span></p>
				</section>

				<section data-transition="fade">
					<h2>Optimization - Gradient Calculation</h2>
<p>$$\epsilon = (y-\hat{y})^2$$</p>

<p>&nbsp;</p>

<p>\(\frac{d\epsilon}{d\hat{y}} =\) <span class="fragment"><span class="fragment">\(-\)</span>\(2(y-\hat{y})\)</span></p>
				</section>

				<section data-transition="fade-in">
					<h2>Optimization - Gradient Calculation</h2>
<p>\(\frac{\partial\hat{y}}{\partial w_0} = x_0\)</p>
<p>\(\frac{d\epsilon}{d\hat{y}} = -2(y-\hat{y})\)</p>

<p>&nbsp;</p>

\(\frac{\partial\epsilon}{\partial w_0} =
	    -2(y-\hat{y})x_0 \)
				</section>

				<section data-transition="fade-in">
					<h2>Optimization - Gradient Calculation</h2>
<p>$$\hat{y} = w_0x_0 + w_1x_1 + w_2x_2 + b\cdot1$$</p>

<p>&nbsp;</p>

\(\frac{\partial\epsilon}{\partial b} =
	    -2(y-\hat{y})\cdot 1 \)
				</section>

				<section>
					<h2>Optimization - Gradient Calculation</h2>
<pre>
	<code data-trim data-noescape class="python">
		delta_y = y - y_hat
		gradient_weights = -2 * delta_y * weights
		gradient_intercept = -2 * delta_y * 1
	</code>
</pre>
				</section>

				<section>
					<h2>Optimization - Parameter Update</h2>
<pre>
	<code data-trim data-noescape class="python">
weights = weights - gradient_weights
intercept = intercept - gradient_intercept
	</code>
</pre>
				</section>


				<section data-transition="fade-out">
					<h2>Optimization - Overshoot</h2>
	<img src="images/latex_generated_images/cost_function_overshoot.png"
	     alt="If we take steps that are too big, we risk overshooting"
			 style="width: 50%;">
					</section>

					<section data-transition="fade">
						<h2>Optimization - Undershoot</h2>
		<img src="images/latex_generated_images/cost_function_undershoot.png"
		     alt="If we take steps that are too small, convergence takes forever"
				 style="width: 50%;">
						</section>


				<section>
					<h2>Optimization - Parameter Update</h2>
<pre>
	<code data-trim data-noescape class="python">
learning_rate = 0.05

weights = weights - \
             learning_rate * gradient_weights
intercept = intercept - \
             learning_rate * gradient_intercept
	</code>
</pre>
				</section>

				<section>
					<h2>Training</h2>
<pre style="font-size:55%;">
	<code data-trim data-noescape class="python">
def training_round(x, y, weights, intercept,
                   alpha=learning_rate):
    # calculate our estimate
    y_hat = model(x, weights, intercept)

    # calculate error
    delta_y = y - y_hat

    # calculate gradients
    gradient_weights = -2 * delta_y * weights
    gradient_intercept = -2 * delta_y

    # update parameters
    weights = weights - alpha * gradient_weights
    intercept = intercept - alpha * gradient_intercept

    return weights, intercept
	</code>
</pre>
				</section>

				<section>
					<h2>Training</h2>
					<pre style="font-size:60%;">
						<code data-trim data-noescape class="python">
NUM_EPOCHS = 100

def train(X, Y):
    # initialize parameters
    weights = np.random.randn(3)
    intercept = 0

    # training rounds
    for i in range(NUM_EPOCHS):
        for (x, y) in zip(X, Y):
            weights, intercept = training_round(x, y,
                                 weights, intercept)
</pre>
</code>
				</section>

				<section>
					<h2>Testing</h2>
					<pre style="font-size:60%;">
						<code data-trim data-noescape class="python">
def test(X_test, Y_test, weights, intercept):
    Y_predicted = model(X_test, weights, intercept)
    error = cost(Y_predicted, Y_test)
    return np.sqrt(np.mean(error))

>>> test(X_test, Y_test, final_weights, final_intercept)
6052.79
</code>
</pre>
				</section>

				<section>
					Uh, wasn't this supposed to be a talk about neural networks?
					Why are we talking about linear regression?
				</section>

		    <section>
					<h2>Surprise! <br/> You've already made <br/> a neural network!</h2>
				</section>

				<section data-transition="fade-out">
					<h2>Linear regression = <br/> Simplest neural network</h2>
					<img src="images/latex_generated_images/linear_regression_as_neural_network.png"
						   style="width:30%"
							 alt = "Linear regression is basically the simplest possible neural network">
				</section>

				<section data-transition="fade-in">
					<h2>Linear regression = <br/> Simplest neural network</h2>
					<img src="images/latex_generated_images/linear_regression_as_neural_network_no_b.png"
						   style="width:30%"
							 alt = "Usually we omit the intercept from this diagram">
				</section>

     </section> <!-- end linear regression section -->

<!-- Linear regression in TensorFlow -->

     <section>
				<section>
					<h2>Once more, with TensorFlow</h2>
				</section>

				<section data-transition="fade-out">
					<h2></h2>
					<ul>
						<li>Inputs</li>
						<li>Model - Parameters</li>
						<li>Model - Operations</li>
						<li>Cost function</li>
						<li>Optimization</li>
						<li>Train</li>
						<li>Test</li>
					</ul>
				</section>

				<section>
					<h2>Inputs &rarr; Placeholders</h2>
<pre>
  <code data-trim data-noescape class="python">
import tensorflow as tf

X = tf.placeholder(tf.float32, [None, 3])
Y = tf.placeholder(tf.float32, [None, 1])
  </code>
</pre>
				</section>

				<section>
					<h2>Parameters &rarr; Variables</h2>
<pre>
  <code data-trim data-noescape class="python">
# create tf.Variable(s)
W = tf.get_variable("weights", [3, 1],
       initializer=tf.random_normal_initializer())
b = tf.get_variable("intercept", [1],
       initializer=tf.constant_initializer(0))
  </code>
</pre>

				</section>

				<section>
					<h2>Operations</h2>
<pre>
  <code data-trim data-noescape class="python">
Y_hat = tf.matmul(X, W) + b
  </code>
</pre>
				</section>

				<section>
					<h2>Cost function</h2>
<pre>
  <code data-trim data-noescape class="python">
cost = tf.reduce_mean(tf.square(Y_hat - Y))
  </code>
</pre>
				</section>

				<section>
					<h2>Optimization</h2>
<pre>
  <code data-trim data-noescape class="python">
learning_rate = 0.05
optimizer = tf.train.GradientDescentOptimizer
               (learning_rate).minimize(cost)
  </code>
</pre>
				</section>

				<section>
					<h2>Training</h2>
<pre>
  <code data-trim data-noescape class="python">
with tf.Session() as sess:
    # initialize variables
    sess.run(tf.global_variables_initializer())

    # train
    for _ in range(NUM_EPOCHS):
        for (X_batch, Y_batch) in get_minibatches(
               X_train, Y_train, BATCH_SIZE):
            sess.run(optimizer,
                     feed_dict={
                         X: X_batch,
                         Y: Y_batch
                     })
  </code>
</pre>
				</section>

				<section data-transition="fade-out">
					<div style="width: 60%; float: left;">

<pre style="font-size: 40%;">
  <code data-trim data-noescape class="python">
# Placeholders
X = tf.placeholder(tf.float32, [None, 3])
Y = tf.placeholder(tf.float32, [None, 1])

# Parameters/Variables
W = tf.get_variable("weights", [3, 1],
       initializer=tf.random_normal_initializer())
b = tf.get_variable("intercept", [1],
       initializer=tf.constant_initializer(0))

# Operations
Y_hat = tf.matmul(X, W) + b

# Cost function
cost = tf.reduce_mean(tf.square(Y_hat - Y))

# Optimization
optimizer = tf.train.GradientDescentOptimizer
               (learning_rate).minimize(cost)

# ------------------------------------------------

# Train
with tf.Session() as sess:
    # initialize variables
    sess.run(tf.global_variables_initializer())

    # run training rounds
    for _ in range(NUM_EPOCHS):
        for X_batch, Y_batch in get_minibatches(
                   X_train, Y_train, BATCH_SIZE):
            sess.run(optimizer,
               feed_dict={X: X_batch, Y: Y_batch})
				</code>
			</pre>
		</div>

					</section>

					<section data-transition="fade">
					<div style="width: 60%; float: left;">

<pre style="font-size: 40%;">
  <code data-trim data-noescape class="python">
# Placeholders
X = tf.placeholder(tf.float32, [None, 3])
Y = tf.placeholder(tf.float32, [None, 1])

# Parameters/Variables
W = tf.get_variable("weights", [3, 1],
       initializer=tf.random_normal_initializer())
b = tf.get_variable("intercept", [1],
       initializer=tf.constant_initializer(0))

# Operations
Y_hat = tf.matmul(X, W) + b

# Cost function
cost = tf.reduce_mean(tf.square(Y_hat - Y))

# Optimization
optimizer = tf.train.GradientDescentOptimizer
               (learning_rate).minimize(cost)

# ------------------------------------------------

# Train
with tf.Session() as sess:
    # initialize variables
    sess.run(tf.global_variables_initializer())

    # run training rounds
    for _ in range(NUM_EPOCHS):
        for X_batch, Y_batch in get_minibatches(
                   X_train, Y_train, BATCH_SIZE):
            sess.run(optimizer,
               feed_dict={X: X_batch, Y: Y_batch})
				</code>
			</pre>
		</div>

			<div style="width: 30%; height: 8em; float: right;">
				<img src="images/blueprint.png"
						 alt="All the code in this section is like a blueprint">
			</div>

      <div style="width: 30%; float: right">
				<pre><code class="python">#-------------</code></pre>
			</div>

			<div style="width: 30%; height: 8em; float: right;">
				<img src="images/rocket.jpg"
				     alt="And this section is like the actual rocket that gets built">
			</div>

					</section>

					<section data-transition="fade">
						<h2>Computation graph</h2>
	<img src="images/latex_generated_images/computation_graph_up_to_y_predicted.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">

					</section>

					<section data-transition="fade">
						<h2>Computation graph</h2>
	<img src="images/latex_generated_images/computation_graph_up_to_error_term.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section data-transition="fade">
						<h2>Forward propagation</h2>
	<img src="images/latex_generated_images/computation_graph_forward_1.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section data-transition="fade">
						<h2>Forward propagation</h2>
	<img src="images/latex_generated_images/computation_graph_forward_2.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section data-transition="fade">
						<h2>Forward propagation</h2>
	<img src="images/latex_generated_images/computation_graph_forward_3.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section data-transition="fade">
						<h2>Forward propagation</h2>
	<img src="images/latex_generated_images/computation_graph_forward_4.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section data-transition="fade">
						<h2>Forward propagation</h2>
	<img src="images/latex_generated_images/computation_graph_forward_5.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section>
						<h2>Forward propagation</h2>
	<pre style="font-size:55%;">
		<code data-trim data-noescape class="python">
	def training_round(x, y, weights, intercept,
	                   alpha=learning_rate):
      # calculate our estimate
      <mark>y_hat = model(x, weights, intercept)</mark>

      # calculate error
      <mark>delta_y = y - y_hat</mark>

      # calculate gradients
      gradient_weights = -2 * delta_y * weights
      gradient_intercept = -2 * delta_y

      # update parameters
      weights = weights - alpha * gradient_weights
      intercept = intercept - alpha * gradient_intercept

      return weights, intercept
		</code>
	</pre>
					</section>


					<section data-transition="fade">
						<h2>Backpropagation</h2>
	<img src="images/latex_generated_images/computation_graph_backprop_goal.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section data-transition="fade">
						<h2>Backpropagation</h2>
	<img src="images/latex_generated_images/computation_graph_backprop_0.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

<!--
					<section data-transition="fade">
						<h2>Backpropagation</h2>
	<img src="images/latex_generated_images/computation_graph_backprop_0.5.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>
-->

					<section data-transition="fade">
						<h2>Backpropagation</h2>
	<img src="images/latex_generated_images/computation_graph_backprop_1.5.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section data-transition="fade">
						<h2>Backpropagation</h2>
	<img src="images/latex_generated_images/computation_graph_backprop_2.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section data-transition="fade">
						<h2>Backpropagation</h2>
	<img src="images/latex_generated_images/computation_graph_backprop_3.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section data-transition="fade">
						<h2>Backpropagation</h2>
	<img src="images/latex_generated_images/computation_graph_backprop_4.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section data-transition="fade">
						<h2>Backpropagation</h2>
	<img src="images/latex_generated_images/computation_graph_backprop_5.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section data-transition="fade">
						<h2>Backpropagation</h2>
	<img src="images/latex_generated_images/computation_graph_backprop_6.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section data-transition="fade">
						<h2>Backpropagation</h2>
	<img src="images/latex_generated_images/computation_graph_backprop_7.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section data-transition="fade">
						<h2>Backpropagation</h2>
	<img src="images/latex_generated_images/computation_graph_backprop_8.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section data-transition="fade">
						<h2>Backpropagation</h2>
	<img src="images/latex_generated_images/computation_graph_backprop_complete.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>

					<section>
						<h2>Backpropagation</h2>
	<pre style="font-size:55%;">
		<code data-trim data-noescape class="python">
	def training_round(x, y, weights, intercept,
	                   alpha=learning_rate):
      # calculate our estimate
      y_hat = model(x, weights, intercept)

      # calculate error
      delta_y = y - y_hat

      # calculate gradients
      <mark>gradient_weights = -2 * delta_y * weights</mark>
      <mark>gradient_intercept = -2 * delta_y</mark>

      # update parameters
      weights = weights - alpha * gradient_weights
      intercept = intercept - alpha * gradient_intercept

      return weights, intercept
		</code>
	</pre>
					</section>

					<section data-transition="fade">
						<h2>Variable Update</h2>
	<img src="images/latex_generated_images/computation_graph_just_before_update.png"
	     alt="Just gradually working through the computation graph..."
	     style="width: 100%;">
					</section>


				<section data-transition="fade">
					<h2>Variable Update</h2>
<img src="images/latex_generated_images/computation_graph_after_update_w0.png"
     alt="Just gradually working through the computation graph..."
     style="width: 100%;">
				</section>

				<section data-transition="fade">
					<h2>Variable Update</h2>
<img src="images/latex_generated_images/computation_graph_after_update.png"
     alt="Just gradually working through the computation graph..."
     style="width: 100%;">
				</section>

				<section>
					<h2>Variable Update</h2>
<pre style="font-size:55%;">
	<code data-trim data-noescape class="python">
def training_round(x, y, weights, intercept,
                   alpha=learning_rate):
    # calculate our estimate
    y_hat = model(x, weights, intercept)

    # calculate error
    delta_y = y - y_hat

    # calculate gradients
    gradient_weights = -2 * delta_y * weights
    gradient_intercept = -2 * delta_y

    # update parameters
    <mark>weights = weights - alpha * gradient_weights</mark>
    <mark>intercept = intercept - alpha * gradient_intercept</mark>

    return weights, intercept
	</code>
</pre>
				</section>

				<section>
					<h2>Variable Update</h2>
<pre style="font-size:55%;">
	<code data-trim data-noescape class="python">
def training_round(x, y, weights, intercept,
                   alpha=learning_rate):
    # calculate our estimate
    y_hat = model(x, weights, intercept)

    # calculate error
    delta_y = y - y_hat

    # calculate gradients
    gradient_weights = -2 * delta_y * weights
    gradient_intercept = -2 * delta_y

    # update parameters
    <mark>weights = weights - alpha * gradient_weights</mark>
    <mark>intercept = intercept - alpha * gradient_intercept</mark>

    return weights, intercept
	</code>
</pre>
				</section>

			<section>
				<h2>Numpy &rarr; TensorFlow</h2>
				<pre>
				  <code data-trim data-noescape class="python">
sess.run(optimizer,
         feed_dict={
             X: X_batch,
             Y: Y_batch
         })
				  </code>
				</pre>
			</section>

				<section>
					<h2>Testing</h2>
<pre>
	<code data-trim data-noescape class="python">
with tf.Session() as sess:
    # train
    # ... (code from above)

    # test
    Y_predicted = sess.run(model,
                    feed_dict = {X: X_test})
    squared_error = tf.reduce_mean(
                 tf.square(Y_test, Y_predicted))

>>> np.sqrt(squared_error)
5967.39
	</code>
</pre>
				</section>
     </section>

   <!-- LOGISTIC REGRESSION -->
		 <section>

				<section>
					<h2>Logistic regression</h2>
				</section>

				<section>
					<h2>Problem</h2>
					<img src="images/mnist.png" alt="MNIST example"
					     style="width: 40%;">
				</section>

				<section>
					<h2>Binary logistic regression - Model</h2>
					<p>
						Take a <b>weighted sum</b> of the features<br/>
						and add a <b>bias term</b> to get the <b>logit</b>.<br/>
						Convert the logit to a <b>probability</b><br/>
						via the <b>logistic-sigmoid function</b>.
					</p>
				</section>

				<section>
					<h2>Binary logistic regression - Model</h2>
					<img src="images/latex_generated_images/logistic_regression_as_neural_network.png"
					     alt="Logistic regression as neural network"
							 style="width: 40%;">
				</section>

				<section>
					<h2>Logistic-sigmoid function</h2>
					<img src="images/latex_generated_images/sigmoid_notitle.png"
					     alt="Logistic-sigmoid function shape"
							 style="width: 50%;">
					<div>$f(x) = \frac{e^x}{1+e^x}$</div>
				</section>

				<section>
					<h2>Classification with logistic regression</h2>
					<img src="images/logistic_regression_binary.png"
					     alt="Binary classification with logistic regression"
							 style="width: 40%;">
				 <p style="font-size: 35%; text-align: left;">Image generated with <a href="http://playground.tensorflow.org">playground.tensorflow.org</a></p>
				</section>

				<section>
					<h2>Model</h2>
					<img src="images/latex_generated_images/multinomial_logistic_regression_as_neural_network_up_to_logits.png"
					     alt="Neural network representation of multi-valued logistic regression"
					     style="width: 30%;">
				</section>

				<section>
					<h2>Softmax</h2>
<pre>
	<code data-trim data-noescape class="python">
Z = np.sum(np.exp(logits))
  </code>
</pre>

<img src="images/latex_generated_images/softmax.png"
		 alt="Softmax"
		 style="width: 60%;">
				</section>

				<section>
					<h2>Model</h2>
					<img src="images/latex_generated_images/multinomial_logistic_regression_as_neural_network.png"
					     alt="Neural network representation of multi-valued logistic regression"
					     style="width: 30%;">
				</section>

				<section>
					<h2>Placeholders</h2>
<pre>
	<code data-trim data-noescape class="python">
# X = vector length 784 (= 28 x 28 pixels)

# Y = one-hot vectors
# digit 0 = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]

X = tf.placeholder(tf.float32, [None, 28*28])
Y = tf.placeholder(tf.float32, [None, 10])
  </code>
</pre>
				</section>

				<section>
					<h2>Variables</h2>

<pre>
	<code data-trim data-noescape class="python">
# Parameters/Variables
W = tf.get_variable("weights", [784, 10],
       initializer=tf.random_normal_initializer())
b = tf.get_variable("bias", [10],
       initializer=tf.constant_initializer(0))
	</code>
</pre>
				</section>

				<section>
					<h2>Operations</h2>
<pre>
	<code data-trim data-noescape class="python">
Y_logits = tf.matmul(X, W) + b
	</code>
</pre>
				</section>

				<section>
					<h2>Cost function</h2>
<pre>
	<code data-trim data-noescape class="python">
cost = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(
         logits=Y_logits, labels=Y))
	</code>
</pre>
				</section>

				<section>
					<h2>Cost function</h2>
Cross Entropy

<p>$H(\hat{y}) = -\sum\limits_i y_i \log(\hat{y}_i)$</p>
				</section>


				<section>
					<h2>Optimization</h2>
<pre>
	<code data-trim data-noescape class="python">
learning_rate = 0.05
optimizer = tf.train.GradientDescentOptimizer
              (learning_rate).minimize(cost)
	</code>
</pre>
				</section>

				<section>
					<h2>Training</h2>
<pre>
	<code data-trim data-noescape class="python">
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for i in range(NUM_EPOCHS):
        for (X_batch, Y_batch) in get_minibatches(
                 X_train, Y_train, BATCH_SIZE):
            sess.run(optimizer,
                     feed_dict={X: X_batch,
                                Y: Y_batch})
	</code>
</pre>
				</section>

				<section>
					<h2>Testing</h2>
<pre>
	<code data-trim data-noescape class="python">
predict = tf.argmax(Y_logits, 1)

with tf.Session() as sess:
    # training code from above

    predictions = sess.run(predict,
                    feed_dict={X: X_test})
    accuracy  = tf.reduce_mean(np.mean(
        np.argmax(Y_test, axis=1) == predictions)

>>> accuracy
0.925
  </code>
</pre>
				</section>

				<section>
					<h2>Deficiencies of linear models</h2>
<img src="images/xor.png" alt="XOR can't be learned by a linear model"
     style="width: 40%;">

		 <p style="font-size: 35%; text-align: left;">Image generated with <a href="http://playground.tensorflow.org">playground.tensorflow.org</a></p>

				</section>

				<section>
					<h2>Deficiencies of linear models</h2>
<img src="images/concentric_circles.png"
     alt="Concentric circles can't be learned by a linear model"
     style="width: 40%;">

		 <p style="font-size: 35%; text-align: left;">Image generated with <a href="http://playground.tensorflow.org">playground.tensorflow.org</a></p>
				</section>

      </section>
<!-- SINGLE HIDDEN LAYER FEEDFORWARD NETWORK -->

     <section>

 			  <section>
					<h2>Let's go deeper!</h2>
				</section>

				<section>
					<h2>Adding another layer</h2>
<img src="images/latex_generated_images/hidden_layer_neural_network_linear.png"
     alt="Adding a hidden layer"
     style="width: 50%;">
				</section>

				<section>
					<h2>Adding another layer - Variables</h2>

	<pre style="font-size: 65%;">
		<code data-trim data-noescape class="python">
HIDDEN_NODES = 128
W1 = tf.get_variable("weights1", [784, HIDDEN_NODES],
       initializer=tf.random_normal_initializer())
b1 = tf.get_variable("bias1", [HIDDEN_NODES],
       initializer=tf.constant_initializer(0))
W2 = tf.get_variable("weights2", [HIDDEN_NODES, 10],
      initializer=tf.random_normal_initializer())
b2 = tf.get_variable("bias2", [10],
      initializer=tf.constant_initializer(0))
		</code>
	</pre>
					</section>

					<section>
						<h2>Adding another layer - operations</h2>
	<pre>
		<code data-trim data-noescape class="python">
hidden   = tf.matmul(X, W1) + b1
y_logits = tf.matmul(hidden, W2) + b2
		</code>
	</pre>
					</section>

				<section>
					<h2>Results</h2>
<table style="text-align: center;">
	<tr>
		<td># hidden layers</td><td>Train accuracy</td><td>Test accuracy</td>
	</tr>
	<tr>
	  <td>0</td><td>93.0</td><td>92.5</td>
	</tr>
	<tr>
		<td>1</td><td>89.2</td><td>88.8</td>
	</tr>
</table>

				</section>

				<section>
					<h2>Is Deep Learning just hype?</h2>

					<p class="fragment"><small>(Well, it's a little bit over-hyped...)</small></p>
				</section>

				<section>
					<h2>Problem</h2>
					<p>
					A linear transformation of a linear
					transformation is <b>still</b> a
					linear transformation!
				</p>

				<p class="fragment">We need to add <b>non-linearity</b> to the system.
				</section>


						<section data-transition="fade-out">
							<h2>Adding non-linearity</h2>
		<img src="images/latex_generated_images/hidden_layer_neural_network_linear.png"
		     alt="Adding a hidden layer that's actually non-linear: before"
		     style="width: 50%;">
						</section>

				<section data-transition="fade">
					<h2>Adding non-linearity</h2>
<img src="images/latex_generated_images/hidden_layer_neural_network_sigmoid.png"
     alt="Adding a hidden layer that's actually non-linear: after"
     style="width: 50%;">
				</section>

				<section>
					<h2>Non-linear activation functions</h2>
					<table>
						<tr>
							<td>
								<img src="images/latex_generated_images/sigmoid.png">
							</td>
							<td>
								<img src="images/latex_generated_images/tanh.png">
							</td>
							<td>
								<img src="images/latex_generated_images/relu.png">
							</td>
						</tr>
					</table>

				</section>

				<section>
					<h2>Adding non-linearity</h2>
<img src="images/latex_generated_images/hidden_layer_neural_network_relu.png"
     alt="Adding a hidden layer with ReLU"
     style="width: 50%;">
				</section>


				<section>
					<h2>Operations</h2>
<pre styl>
	<code data-trim data-noescape class="python">
hidden   = tf.nn.relu(tf.matmul(X, W1) + b1)
y_logits = tf.matmul(hidden, W2) + b2
	</code>
</pre>
				</section>

				<section>
					<h2>Results</h2>
<table  style="text-align: center;">
	<tr>
		<td># hidden layers</td><td>Train accuracy</td><td>Test accuracy</td>
	</tr>
	<tr>
	  <td>0</td><td>93.0</td><td>92.5</td>
	</tr>
	<tr>
		<td>1</td><td>97.9</td><td>95.2</td>
	</tr>
</table>
				</section>

				<section>
					<h2>What the hidden layer bought us</h2>
<img src="images/xor_4hidden.png" alt="XOR can be learned with a non-linear hidden layer"
     style="width: 40%;">

<p style="font-size: 35%; text-align: left;">Image generated with <a href="http://playground.tensorflow.org">playground.tensorflow.org</a></p>

				</section>

				<section>
					<h2>What the hidden layer bought us</h2>
<img src="images/concentric_circles_3hidden.png"
     alt="Concentric circles can be learned with a non-linear hidden layer"
     style="width: 40%;">

 <p style="font-size: 35%; text-align: left;">Image generated with <a href="http://playground.tensorflow.org">playground.tensorflow.org</a></p>
				</section>

				<section data-transition="fade-out">
					<h2>Adding hidden neurons</h2>

<img src="images/clusters_2neurons.png"
     alt="Effect of adding hidden neurons"
     style="width: 40%;">

<p>2 hidden neurons</p>

 <p style="font-size: 35%; text-align: left;">Image generated with <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">ConvNetJS by Andrej Karpathy</a></p>
				</section>

				<section data-transition="fade">
					<h2>Adding hidden neurons</h2>

<img src="images/clusters_3neurons.png"
     alt="Effect of adding hidden neurons"
     style="width: 40%;">

		 <p>3 hidden neurons</p>

		 <p style="font-size: 35%; text-align: left;">Image generated with <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">ConvNetJS by Andrej Karpathy</a></p>
				</section>

				<section data-transition="fade">
					<h2>Adding hidden neurons</h2>

<img src="images/clusters_4neurons.png"
     alt="Effect of adding hidden neurons"
     style="width: 40%;">

		 <p>4 hidden neurons</p>

		 <p style="font-size: 35%; text-align: left;">Image generated with <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">ConvNetJS by Andrej Karpathy</a></p>
				</section>

				<section data-transition="fade">
					<h2>Adding hidden neurons</h2>

<img src="images/clusters_5neurons.png"
     alt="Effect of adding hidden neurons"
     style="width: 40%;">

		 <p>5 hidden neurons</p>

		 <p style="font-size: 35%; text-align: left;">Image generated with <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">ConvNetJS by Andrej Karpathy</a></p>

				</section>

				<section data-transition="fade">
					<h2>Adding hidden neurons</h2>

				<img src="images/clusters_5neurons_mapping.png"
				     alt="The hidden layer transforms our old features into a new feature space..."
				     style="width: 80%;">

						 <p style="font-size: 35%; text-align: left;">Image generated with <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">ConvNetJS by Andrej Karpathy</a></p>
				</section>

				<section data-transition="fade">
					<h2>Adding hidden neurons</h2>

				<img src="images/clusters_5neurons_mapping_with_boundary.png"
				     alt="...where a linear classifier can classify our points"
				     style="width: 80%;">

						 <p style="font-size: 35%; text-align: left;">Image generated with <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">ConvNetJS by Andrej Karpathy</a></p>
				</section>

				<section>
					<h2>Universal approximation theorem</h2>
					A <b>feedforward network</b> with a <b>single hidden layer</b>
					containing a finite number of neurons <b>can approximate</b>
					(basically) <b>any interesting function</b>
				</section>

			</section>
<!-- DEEP LEARNING -->
			<section>

				<section>
					<h2>Are we deep learning yet?</h2>
					<p class="fragment">No!</p>
				</section>

				<section>
					<h2>Operations</h2>
<pre style="font-size: 68%;">
	<code data-trim data-noescape class="python">
hidden_1 = tf.nn.relu(tf.matmul(X, W1) + b1)
hidden_2 = tf.nn.relu(tf.matmul(hidden_1, W2) + b2)
y_logits = tf.matmul(hidden_2, W3) + b3
	</code>
</pre>
				</section>

				<section>
					<h2>Why go deep?</h2>
					3 reasons:
					<ul>
						<li class="fragment">Deeper networks are <b>more powerful</b></li>
						<li style="visibility: hidden;">Narrower networks are <b>less prone to overfitting</b></li>
						<li style="visibility: hidden;">Deeper networks learn <b>hierarchical feature representations</b></li>
					</ul>
				</section>

				<section>
					<h2>More powerful</h2>
					<img src="images/latex_generated_images/shallow_vs_deep_power.png"
					     alt="Some functions can be approximated only by a layer-k network
							      with an exponential number of hidden neurons, but with a
										layer (k+1) network with only a polynomial number of hidden
										neurons"
							 style="width: 70%;">
				</section>

				<section>
					<h2>Why go deep?</h2>
					3 reasons:
					<ul>
						<li>Deeper networks are <b>more powerful</b></li>
						<li>Narrower networks are <b>less prone to overfitting</b></li>
						<li style="visibility: hidden;">Deeper networks learn <b>hierarchical feature representations</b></li>
					</ul>
				</section>

				<section>
					<h2>Overfitting</h2>
					<img src="images/overfitting.png"
					     alt="Illustration of overfitting"
							 style="width: 40%">
				</section>

				<section>
					<h2>Less prone to overfitting</h2>
					<img src="images/latex_generated_images/autoencoder.png"
					     alt="When data is funnelled through a narrow layer,
							      the network is forced to select a good, compressed
										representation. One application of this is
										the autoencoder network"
							 style="width: 70%;">
				</section>

				<section>
					<h2>Why go deep?</h2>
					3 reasons:
					<ul>
						<li>Deeper networks are <b>more powerful</b></li>
						<li>Narrower networks are <b>less prone to overfitting</b></li>
						<li>Deeper networks learn <b>hierarchical feature representations</b></li>
					</ul>
				</section>

				<section>
					<h2>Learns hierarchical representations</h2>
					<img src="images/goodfellow_representation_learning.png"
						   alt="Convolutional neural networks learn hierarchical representations"
							 style="width:50%;">
					 <p class="fragment">&rarr; End-to-end learning</p>
					 <p style="font-size: 35%; text-align: left;">Image source: Goodfellow, Bengio, and Courville (2016) <a href="http://www.deeplearningbook.org/">(The) Deep Learning (Book)</a></p>
				</section>

				<section>
					<h2>Why go deep?</h2>
					3 reasons:
					<ul>
						<li>Deeper networks are <b>more powerful</b></li>
						<li>Narrower networks are <b>less prone to overfitting</b></li>
						<li>Deeper networks learn <b>hierarchical feature representations</b></li>
					</ul>
					<p>So let's go deeper!</p>
				</section>

				<section>
					<h2>Results</h2>
<table>
	<tr>
		<td># hidden layers</td><td>Train accuracy</td><td>Test accuracy</td>
	</tr>
	<tr>
	  <td>0</td><td>93.0</td><td>92.5</td>
	</tr>
	<tr>
		<td>1</td><td>97.9</td><td>95.2</td>
	</tr>
	<tr>
		<td>2</td><td>98.0</td><td>94.2</td>
	</tr>
</table>
				</section>

				<section>
					<h2>Results</h2>
					<img src="images/effect_of_depth.png"
					     alt="Effect of increasing depth of network"
							 style="width: 65%;">
			 <p style="font-size: 35%; text-align: left;">Image source: Goodfellow, Bengio, and Courville (2016) <a href="http://www.deeplearningbook.org/">(The) Deep Learning (Book)</a></p>
				</section>

				<section>
					<h2>Overfitting</h2>
					<img src="images/train_test_overfitting.svg"
					     alt="It's useful to plot train and test error
							      against epochs and model complexity
										to diagnose over- and under-fitting"
							 style="width: 50%;">
				</section>

      </section>
<!-- REGULARIZATION -->
			<section>

				<section>
					<h2>Regularization</h2>
				</section>

				<section>
					<h2>Regularization</h2>
					Put the brakes on the <b>training data</b>
					by enforcing constraints on <b>weights</b>.
				</section>

				<section>
					<h2>Regularization</h2>
					<p><b>L2 regularization:</b> weights should be small.</p>
					$L = \sum{w_i^2}$
				</section>

				<section>
					<h2>L2 Regularization in TensorFlow</h2>
<pre style="font-size: 68%;">
	<code data-trim data-noescape class="python">
cost += REGULARIZATION_CONSTANT * \
          (tf.nn.l2_loss(W1) +
           tf.nn.l2_loss(W2) +
           tf.nn.l2_loss(W3))
	</code>
</pre>
				</section>

<!-- haven't gotten this method to work yet
				<section>
					<h2>L2 Regularization in TensorFlow</h2>
<pre style="font-size: 60%;">
	<code data-trim data-noescape class="python">
# Alternative technique
W1 = tf.get_variable("weights", [784, HIDDEN_NODES],
      initializer=tf.random_normal_initializer(),
      <mark>regularizer=tf.contrib.layers.l2_regularizer(</mark>
                                        <mark>scale=0.01)</mark>)
# ditto with W2, W3...

cost_function = data_loss + tf.get_collection(
                    tf.GraphKeys.REGULARIZATION_LOSSES)
	</code>
</pre>
				</section>
-->

<section>
	<h2>Results</h2>
	<table>
		<tr>
			<td>Regularization</td><td>Train accuracy</td><td>Test accuracy</td>
		</tr>
		<tr>
			<td>None</td><td>95.5</td><td>92.9</td>
		</tr>
		<tr>
			<td>L2</td><td>95.1</td><td>95.1</td>
		</tr>
	</table>
</section>

				<section data-transition="fade-out">
					<h2>Dropout - Train</h2>
<img src="images/latex_generated_images/dropout_before.png"
     alt="Before dropout: using all hidden nodes"
		 style="width: 60%;">
				</section>

				<section data-transition="fade">
					<h2>Dropout - Train</h2>
<img src="images/latex_generated_images/dropout.png"
     alt="Before dropout: knock out half the hidden nodes"
		 style="width: 60%;">
				</section>

				<section data-transition="fade">
					<h2>Dropout - Train</h2>
<img src="images/latex_generated_images/dropout_train.png"
     alt="Before dropout: knock out half the hidden nodes"
		 style="width: 60%;">
				</section>

				<section>
					<h2>Dropout: why it works</h2>
<ul>
	<li>"Averaging" over several models</li>
  <li class="fragment">Forces redundancy of useful features</li>
	<li class="fragment">No conspiracies! Hidden neurons must be individually useful</li>
</ul>
				</section>

				<section data-transition="fade-out">
					<h2>Dropout - Test</h2>
<img src="images/latex_generated_images/dropout_before.png"
     alt="Before dropout: using all hidden nodes"
		 style="width: 60%;">
				</section>

				<section data-transition="fade">
					<h2>Dropout - Train</h2>
<img src="images/latex_generated_images/dropout_2x.png"
     alt="Before dropout: in training, bump up the rest by 2x"
		 style="width: 60%;">
				</section>

				<section>
					<h2>Dropout in TensorFlow</h2>
<pre>
	<code data-trim data-noescape class="python">
# add a new placeholder
<mark>keep_prob = tf.placeholder(tf.float32)</mark>

# add a step to the model
hidden   = tf.nn.relu(tf.matmul(X, w0) + b0)
<mark>dropout  = tf.nn.dropout(hidden, keep_prob)</mark>
y_logits = tf.nn.relu(tf.matmul(dropout, w1) + b1)
  </code>
</pre>
				</section>

				<section>
					<h2>Dropout in TensorFlow</h2>
<pre>
	<code data-trim data-noescape class="python">

with tf.Session() as sess:
    # ... init, then train:
    for _ in range(NUM_EPOCHS <mark>* 2</mark>):
        for (X_batch, Y_batch) in get_minibatches(
              X_train, Y_train, BATCH_SIZE):
            sess.run(optimizer,
                     feed_dict={
                         X: X_batch, Y: Y_batch,
                         <mark>keep_prob: 0.5</mark>
                     })

    # test
    sess.run(predict, feed_dict={X: X_test,
                            <mark>keep_prob: 1.0</mark>})
  </code>
</pre>
				</section>

				<section>
					<h2>Results</h2>
					<table>
						<tr>
							<td>Regularization</td><td>Train accuracy</td><td>Test accuracy</td>
						</tr>
						<tr>
							<td>None</td><td>95.5</td><td>92.9</td>
						</tr>
						<tr>
							<td>L2</td><td>95.1</td><td>95.1</td>
						</tr>
						<tr>
							<td>Dropout</td><td>93.3</td><td>93.1</td>
						</tr>
					</table>
				</section>

    </section>
<!-- end section on regularization -->
<!-- start conclusion -->

    <section>
				<section>
					<h2>Where to from here?</h2>
				</section>

				<section data-transition="fade-out">
					<h2>A guide to further exploration</h2>
					<ul>
						<li>Placeholders</li>
						<li>Model - Variables</li>
						<li>Model - Operations</li>
						<li>Cost function</li>
						<li>Optimization</li>
						<li>Regularization</li>
					</ul>
				</section>

				<section data-transition="fade-in">
					<h2>A guide to further exploration</h2>
					<ul>
						<li style="color: #A9A9A9;">Placeholders</li>
						<li>Model - Variables</li>
						<li>Model - Operations</li>
						<li style="color: #A9A9A9;">Cost function</li>
						<li>Optimization</li>
						<li>Regularization</li>
					</ul>
				</section>

				<section>
					<h2>Model - Variables</h2>
					<p>
					<img src="images/nnz_mlp.png"
					     style="width: 20%;">
					</p>
					<p># layers, # neurons / layer</p>
					<p style="font-size: 35%; text-align: left;">Image source: Fjodor van Veen (2016) <a href="http://www.asimovinstitute.org/neural-network-zoo/">Neural Network Zoo</a></p>
				</section>

				<section>
					<h2>Model - Variables</h2>
					<ul>
						<li>tf.random_normal_initializer</li>
						<li>tf.random_uniform_initializer</li>
						<li>tf.truncated_normal_initializer</li>
						<li>tf.constant_initializer</li>
						<li>tf.contrib.layers.xavier_initializer</li>
					</ul>
				</section>

				<section>
					<h2>Model - Operations</h2>
					<p>
					<img src="images/nnz_mlp.png"
					     style="width: 20%;">
					</p>
					<p>Activation functions: ReLU, tanh, leaky RELU, Maxout...</p>
					<p style="font-size: 35%; text-align: left;">Image source: Fjodor van Veen (2016) <a href="http://www.asimovinstitute.org/neural-network-zoo/">Neural Network Zoo</a></p>
				</section>

				<section>
					<h2>Model</h2>
					<img src="images/nnz_cnn.png" style="vertical-align: top; width: 34%;">
					<img src="images/nnz_rnn.png" style="vertical-align: top; width: 40%;">
					Convolutional neural networks (images)<br/>
					Recurrent neural networks (sequences &amp; time series)

					<p style="font-size: 35%; text-align: left;">Image source: Fjodor van Veen (2016) <a href="http://www.asimovinstitute.org/neural-network-zoo/">Neural Network Zoo</a></p>
				</section>

				<section>
					<h2>Optimization</h2>
					<img src="images/radford_sgd_comparison.gif">
					<p>Try Adam</p>
					<p style="font-size: 35%; text-align: left;">Image source: <a href="https://www.reddit.com/r/MachineLearning/comments/2gopfa/visualizing_gradient_optimization_techniques/cklhott/">Alec Radford</a></p>
				</section>

				<section>
					<h2>Optimization &amp; Regularization</h2>
					<ul>
						<li>L1, L2 regularization</li>
						<li>Dropout</li>
						<li>Batch normalization</li>
						<li>Layer normalization</li>
					</ul>
				</section>

				<section>
					<h2>Other toolkits</h2>
					<ul>
						<li>Torch (PyTorch)</li>
						<li>Caffe</li>
						<li>mxnet</li>
						<li>DyNet</li>
						<li>Many others...</li>
					</ul>
				</section>

				<section>
					<h2>Keras</h2>

					<!-- analogy -->
$$
\begin{align*}
\textrm{numpy} &: \textrm{scikit-learn} \\
&:: \\
\textrm{TensorFlow} &: \textrm{Keras}
\end{align*}
$$
				</section>

				<section>
					<h2>Keras</h2>

<pre style="font-size: 58%;">
	<code data-trim data-noescape class="python">
from keras.models import Sequential
model = Sequential()

model.add(Dense(input_dim=784, units=128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=10, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer=keras.optimizers.SGD(lr=0.05),
              metrics=['accuracy'])

model.fit(X_train, Y_train, epochs=100, batch_size=120)
model.evaluate(X_test, Y_test)
	</code>
</pre>
				</section>

				<section>
					<h2>Final thoughts</h2>

					<ul>
					  <li class="fragment">If you're familiar with traditional ML,<br>you can do deep learning!</li>
						<li class="fragment">But you'll need data. Lots of it.</li>
					  <li class="fragment">So try traditional ML first.</li>
						<li class="fragment">Go forth and experiment!</li>
						<li class="fragment">Thank you!</li>
				  </ul>
					<p>&nbsp;</p>
					<p>Slides: michelleful.github.io/PyCon2017</p>
				</section>
      </section> <!-- end conclusion -->

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: true,
				transition: 'slide',

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 				  { src: 'plugin/math/math.js', async: true }
				]
			});
		</script>
	</body>
</html>
