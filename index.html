<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>A gentle introduction to Deep Learning with Tensorflow - PyCon 2017</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/beige.css">
		<link rel="stylesheet" href="css/custom.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/googlecode.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<!-- Title slide -->
				<section>
					<h2>A gentle introduction to deep learning with TensorFlow</h2>
					<p>Michelle Fullwood<br />
					@michelleful</p>
					<p>PyCon 2017</p>
				</section>

				<section>
					TODO: graph

					Deep learning learning curve
				</section>

				<section>
					<table style="border-collapse:collapse;">
						<tr>
							<td style="border: none; text-align: center">
								<img src="images/hammer.png"
									   style="width: 50%; display: block;
										        margin-left: auto; margin-right: auto;">
							</td>
							<td style="border: none; text-align: center;">
								<div class="fragment" data-fragment-index="1">
								<img src="images/tensorflow_logo_big.png"
						         style="width: 55%; display: block;
						                margin-left: auto; margin-right: auto">
								</div>
							</td>
						</tr>
						<tr>
							<td style="text-align: center;">Traditional machine learning</td>
							<td style="text-align: center;"><div class="fragment" data-fragment-index="1">Deep learning</div></td>
						</tr>
					</table>
				</section>

				<section>
					<h2>Linear Regression
				</section>

				<section>
					<h2>Linear Regression</h2>
					<img src="images/regression_feature_floor_area.png"
						   style="width:14%; margin:2%"
							 alt = "Feature: floor area"
							 class="fragment" data-fragment-index="3">
					<img src="images/regression_feature_distance.png"
							 style="width:18%; margin:2%;"
							 alt="Feature: distance from public transportation"
							 class="fragment" data-fragment-index="4">
					<img src="images/regression_feature_number_of_rooms.png"
							 alt="Feature: number of bedrooms"
					     style="width:14%; margin:2%"
							 class="fragment" data-fragment-index="5">
				  <img src="images/right_arrow.png"
							 alt="are predictors for"
	 					   style="width:15%"
							 class="fragment" data-fragment-index="2">
					<img src="images/regression_target_house_price.png"
						   alt="Target value for regression: house price"
					     style="width:18%;">
				</section>

				<section>
					<h2>Variables</h2>
					<p>$$\left[ \begin{array}{ccc}
						1250 & 350 & 3 \end{array} \right] \rightarrow 345000$$</p>
				</section>

				<section>
					<h2>Variables</h2>
					<pre>
						<code data-trim data-noescape>
							import numpy as np

							x = np.array([1250, 350, 3])
							y = 345000
						</code>
					</pre>
				</section>

				<section>
					<h2>Model</h2>
					<p>
						Multiply each <b>feature</b> by a <b>weight</b>.<br/>
						Add an <b>intercept</b> to get our final <b>estimate</b>.
					</p>
				</section>

				<section>
					<h2>Model</h2>
					<img src="images/latex_generated_images/linear_regression_2d_nopoints.png"
						   style="width:50%"
							 alt = "Linear regression is a straight line with one feature">
				</section>

				<section>
					<h2>Model - Parameters</h2>
					<pre>
						<code data-trim data-noescape>
weights = np.array([300, -10, -1])
intercept = 26497
						</code>
					</pre>
				</section>

				<section>
					<h2>Model - Operations</h2>
					<img src="images/latex_generated_images/dot_product.png"
						   style="width:50%"
							 alt = "Use dot product to accomplish the weight multiplication step">
				</section>

				<section>
					<h2>Model - Operations</h2>
					<pre>
						<code data-trim data-noescape>
def model(x, weights, intercept):
    return x.dot(weights) + intercept

>>> y_hat = model(x, weights, intercept)
>>> y_hat
345000
						</code>
					</pre>
				</section>

				<section>
					<h2>Model - Operations</h2>
<img src="images/latex_generated_images/linear_regression_2d_1point.png"
	   style="width:50%"
		 alt = "It's easy to match just one point">
				</section>

				<section>
					<h2>Model - Operations</h2>
					<img src="images/latex_generated_images/linear_regression_2d_multiplepoints.png"
						   style="width:50%"
							 alt = "But the chances it works on more points are...minuscule">
				</section>

				<section>
					<h2>Model - Variables</h2>
<pre>
<code data-trim data-noescape>
X = np.array([
		[??, ??, ??],
		[??, ??, ??],
		[??, ??, ??]
])

Y = np.array([??, ??, ??])
</code>
</pre>
				</section>

				<section>
					<h2>Interlude: scaling</h2>
		<pre>
			<code data-trim data-noescape>
import sklearn

# center around zero and divide by stddev
scalerX = sklearn.preprocessing.StandardScaler()
scalerY = sklearn.preprocessing.StandardScaler()

X = scalerX.fit_transform(X)
Y = scalerY.fit_transform(Y)
			</code>
		</pre>

				</section>

				<section>
					<h2>Model - Variables</h2>
<pre>
<code data-trim data-noescape>
>>> X  # fake numbers!
np.array([
	[??, ??, ??],
	[??, ??, ??],
	[??, ??, ??]
])

>>> Y
np.array([??, ??, ??])
</code>
</pre>
				</section>


				<section>
					<h2>Model - Parameters</h2>
<!-- no change -->
					<pre>
						<code data-trim data-noescape>
weights = np.array([??, ??, ??])
intercept = ??
						</code>
					</pre>
				</section>

				<section data-transition="fade">
					<h2>Model - Operations</h2>
<img src="images/latex_generated_images/matrix_mult_1.png"
	   style="width:70%"
		 alt = "Matrix multiplication: the two matrices to be multiplied">
				</section>

				<section data-transition="fade">
					<h2>Model - Operations</h2>
<img src="images/latex_generated_images/matrix_mult_2.png"
		 style="width:70%"
		 alt = "Matrix multiplication: dot product the row and column of the desired entry">
				</section>

				<section data-transition="fade">
					<h2>Model - Operations</h2>
<img src="images/latex_generated_images/matrix_mult_3.png"
		 style="width:70%"
		 alt = "Matrix multiplication: first result">
				</section>

				<section data-transition="fade">
					<h2>Model - Operations</h2>
<img src="images/latex_generated_images/matrix_mult_4.png"
		 style="width:70%"
		 alt = "Matrix multiplication: second result">
				</section>

				<section>
					<h2>Model - Operations</h2>
					<pre>
						<code data-trim data-noescape>
def model(X, weights, intercept):
    return X.dot(weights) + intercept

>>> Y_hat = model(X, weights, intercept)
>>> Y_hat
[345000, ??, ??]
						</code>
					</pre>
				</section>

				<section data-transition="fade-out">
					<h2>Model - Cost function</h2>
					<img src="images/latex_generated_images/linear_regression_2d_multiplepoints.png"
						   style="width:50%"
							 alt = "Okay so we had a pretty bad fit...let's measure it">
				</section>

				<section data-transition="fade">
					<h2>Model - Cost function</h2>
					<img src="images/latex_generated_images/linear_regression_2d_multiplepoints_witherrorlines.png"
						   style="width:50%"
							 alt = "Drop a line from the actual y to the estimated y_hat">
				</section>

				<section data-transition="fade-in">
					<h2>Model - Cost function</h2>
					<img src="images/latex_generated_images/linear_regression_2d_multiplepoints_betterfit.png"
						   style="width:50%"
							 alt = "Drop a line from the actual y to the estimated y_hat">
				</section>

				<section>
					<h2>Cost function</h2>
					<pre>
						<code data-trim data-noescape>
def cost_function(Y_hat, Y):
    return np.sum((Y_hat - Y)**2)
						</code>
				</section>

				<section>
					<h2>Optimization</h2>
<p>Adjust <b>parameters</b> to minimize <b>cost</b>.</p>
				</section>

				<section>
					<h2>Optimization</h2>
Graph: Parabola with two tangents
				</section>

				<section data-transition="fade-out">
					<h2>Optimization - Gradient Calculation</h2>
<p>$$\hat{y} = w_0x_0 + w_1x_1 + w_2x_2 + b$$
$$\epsilon = (y-\hat{y})^2$$</p>

<p>&nbsp;</p>

<p><b>Goal:</b> \(\frac{d\epsilon}{dw_i}, \frac{d\epsilon}{db}\)</p>
				</section>

				<section data-transition="fade">
					<h2>Optimization - Gradient Calculation</h2>
<p>$$\hat{y} = w_0x_0 + w_1x_1 + w_2x_2 + b$$</p>

<p>&nbsp;</p>

<p>\(\frac{\partial\hat{y}}{\partial w_0} =\)<span class="fragment">\( x_0\)</span></p>
				</section>

				<section data-transition="fade">
					<h2>Optimization - Gradient Calculation</h2>
<p>$$\epsilon = (y-\hat{y})^2$$</p>

<p>&nbsp;</p>

<p>\(\frac{d\epsilon}{d\hat{y}} =\) <span class="fragment"><span class="fragment">\(-\)</span>\(2(y-\hat{y})\)</span></p>
				</section>


				<section data-transition="fade">
					<h2>Optimization - Gradient Calculation</h2>
<p>\(\frac{\partial\hat{y}}{\partial w_0} = x_0\)</p>
<p>\(\frac{d\epsilon}{d\hat{y}} = -2(y-\hat{y})\)</p>

<p>&nbsp;</p>

<p><b>Chain rule:</b> \(\frac{\partial\epsilon}{\partial x_0} =
	    \frac{d\epsilon}{d\hat{y}}\frac{\partial\hat{y}}{\partial w_0} \)
				</section>

				<section data-transition="fade-in">
					<h2>Optimization - Gradient Calculation</h2>
<p>\(\frac{\partial\hat{y}}{\partial w_0} = x_0\)</p>
<p>\(\frac{d\epsilon}{d\hat{y}} = -2(y-\hat{y})\)</p>

<p>&nbsp;</p>

\(\frac{\partial\epsilon}{\partial x_0} =
	    -2(y-\hat{y})x_0 \)
				</section>

				<section data-transition="fade-in">
					<h2>Optimization - Gradient Calculation</h2>
<p>$$\hat{y} = w_0x_0 + w_1x_1 + w_2x_2 + b\cdot1$$</p>

<p>&nbsp;</p>

\(\frac{\partial\epsilon}{\partial b} =
	    -2(y-\hat{y})\cdot 1 \)
				</section>

				<section>
					<h2>Optimization - Gradient Calculation</h2>
<pre>
	<code data-trim data-noescape>
		delta_y = y - y_hat
		delta_weights = -2 * delta_y * weights
		delta_intercept = -2 * delta_y * 1
	</code>
</pre>
				</section>

				<section>
					<h2>Optimization - Learning Rate</h2>
					TODO
				</section>

				<section>
					<h2>Optimization - Parameter Update</h2>
<pre>
	<code data-trim data-noescape>
learning_rate = 0.05

weights = weights - \
             learning_rate * delta_weights
intercept = intercept - \
             learning_rate * delta_intercept
	</code>
</pre>
				</section>

				<section>
					<h2>Training</h2>
<pre style="font-size:60%;">
	<code data-trim data-noescape>
def training_round(X, Y, weights, intercept,
                   alpha=learning_rate):
    # calculate our estimate, y_hat
    Y_hat = model(X, weights, intercept)

    # calculate error
    delta_Y = Y - Y_hat  # TODO: check for batch

    # calculate gradients
    delta_weights = -2 * delta_y * weights
    delta_intercept = -2 * delta_y

    # update parameters
    weights = weights - alpha * delta_weights
    intercept = intercept - alpha * delta_intercept

    return weights, intercept
	</code>
</pre>
				</section>

				<section>
					<h2>Training</h2>
					<pre style="font-size:60%;">
						<code data-trim data-noescape>
def train(X, Y):
    # initialize weights to something small and non-zero
    # using a recipe from http://cs231n.github.io/neural-networks-2/
    weights = np.random.randn(len(Y)) * sqrt(2.0/len(Y))

    for i in range(100):
        weights, intercept = training_round(X, Y,
                               weights, intercept)
</pre>
</code>
				</section>

				<section>
					<h2>Testing</h2>
					<pre style="font-size:60%;">
						<code data-trim data-noescape>
# don't forget to scale!
X_test = scalerX.transform(X_test)
Y_test = scalerY.transform(Y_test)

def test(X_test, Y_test, final_weights, final_intercept):
    Y_hat_test = model(X_test, final_weights,
                               final_intercept)
    return cost_function(Y_hat_test, Y_test) / len(Y_test)

>>> accuracy = test(X_test, Y_test,
                    final_weights, final_intercept)
>>> accuracy
???
</code>
</pre>
				</section>

		    <section>
					<h2>Surprise! <br/> You've made <br/> a neural network!</h2>
				</section>

				<section data-transition="fade-out">
					<h2>Linear regression = <br/> Simplest neural network</h2>
					<img src="images/latex_generated_images/linear_regression_as_neural_network.png"
						   style="width:30%"
							 alt = "Linear regression is basically the simplest possible neural network">
				</section>

				<section data-transition="fade-in">
					<h2>Linear regression = <br/> Simplest neural network</h2>
					<img src="images/latex_generated_images/linear_regression_as_neural_network_no_b.png"
						   style="width:30%"
							 alt = "Usually we omit the intercept from this diagram">
				</section>

				<section>
					<h2>Once more, with TensorFlow</h2>
				</section>

				<section>
					<h2>Variables/Placeholders</h2>
<pre>
  <code data-trim data-noescape>
import tensorflow as tf

X = tf.placeholder(tf.float32, [None, 3])
Y = tf.placeholder(tf.float32, [None, 1])
  </code>
</pre>
				</section>

				<section>
					<h2>Model - parameters</h2>

<pre>
  <code data-trim data-noescape>
weights   = tf.Variable(tf.zeros([3, 1]))
intercept = tf.Variable(tf.zeros([1, 1]))
  </code>
</pre>

				</section>

				<section>
					<h2>Model - operations</h2>
<pre>
  <code data-trim data-noescape>
Y_hat = tf.matmul(X, weights) + intercept
  </code>
</pre>
				</section>

				<section>
					<h2>Cost function</h2>
<pre>
  <code data-trim data-noescape>
cost = tf.reduce_mean(tf.square(Y_hat - Y))
  </code>
</pre>
				</section>

				<section>
					<h2>Optimization</h2>
<pre>
  <code data-trim data-noescape>
learning_rate = 0.05
optimizer = tf.train.GradientDescentOptimizer
               (learning_rate).minimize(cost)
  </code>
</pre>
				</section>

				<section>
					<h2>Training</h2>
<pre>
  <code data-trim data-noescape>
with tf.Session() as sess:
    # initialize variables
    sess.run(tf.global_variables_initializer())

    for epoch in range(100):
        X_batch, Y_batch = random_minibatch(X, Y)
        sess.run(optimizer,
                 feed_dict={
                     X: X_batch,
                     Y: Y_batch
                 })
  </code>
</pre>
				</section>

				<section>

					<div style="width: 60%; float: left;">

<pre style="font-size: 40%;">
  <code data-trim data-noescape>
# Placeholders
X = tf.placeholder(tf.float32, [None, 3])
Y = tf.placeholder(tf.float32, [None, 1])

# Parameters
weights   = tf.Variable(tf.zeros([3, 1]))
intercept = tf.Variable(tf.zeros([1, 1]))

# Operations
Y_hat = tf.matmul(X, weights) + intercept

# Cost function
cost = tf.reduce_mean(tf.square(Y_hat - Y))

# Optimization
optimizer = tf.train.GradientDescentOptimizer
               (learning_rate).minimize(cost)
	</code>
</pre>

								</div>
								<div style="width: 30%; height: 8em; float: right;">
									<img src="images/blueprint.png"
									     alt="All the code in this section is like a blueprint">
								</div>

								<div style="width: 70%; float: left;">

			<pre style="font-size: 40%;">
			  <code data-trim data-noescape>
# train
with tf.Session() as sess:
    # initialize variables
    sess.run(tf.global_variables_initializer())

    # run training rounds
    for epoch in range(100):
        X_batch, Y_batch = random_minibatch(X, Y)
        sess.run(optimizer,
                 feed_dict={X: X_batch, Y: Y_batch})
				</code>
			</pre>

											</div>
											<div style="width: 30%; height: 8em; float: right;">
												<img src="images/rocket.jpg"
												     alt="And this section is like the actual rocket that gets built">
											</div>

					</section>

				<section>
					<h2>Computation graph</h2>
Illustrate the computation graph - two parts to it, first
computing y' and then computing the error
				</section>

				<section>
					<h2>Train loop - Forward propagation</h2>
Show the numbers
				</section>

				<section>
					<h2>Train loop - Backward propagation</h2>
Show the numbers
				</section>

				<section>
					<h2>Train loop - Update</h2>

				</section>

				<section>
					<h2>Testing</h2>
				</section>

				<section>
					<h2>Logistic regression
				</section>

				<section>
					<h2>Logistic regression</h2>
illustrate the new problem
				</section>

				<section>
					<h2>Logistic regression</h2>
illustrate the solution of adding a function onto the end
why not just treat it like linear regression?
				</section>

				<section>
					<h2>What needs changing?</h2>
				</section>

				<section>
					<h2>Variables/Placeholders</h2>
					No change
				</section>

				<section>
					<h2>Model - parameters</h2>
					No change, but different interpretation
				</section>

				<section>
					<h2>Model - operations</h2>
					Changed
				</section>

				<section>
					<h2>Cost function</h2>
					Changed
				</section>

				<section>
					<h2>Optimization</h2>
					Same
				</section>

				<section>
					<h2>Training</h2>
					Same
				</section>

				<section>
					<h2>Testing</h2>
					Same
				</section>

				<section>
					<h2>Neural network interpretation</h2>
				</section>

				<section>
					<h2>Activation functions</h2>
					* Identity function: linear regression
					* Step function 0/1: perceptron
					* Logistic function: logistic regression
				</section>

				<section>
					<h2>Multi-class logistic regression</h2>
				</section>

				<section>
					<h2>Let's go deeper!</h2>
				</section>

				<section>
					<h2>Adding another layer</h2>
					diagram
				</section>

				<section>
					<h2>Adding another layer</h2>
					Code
				</section>

				<section>
					<h2>Results</h2>
					Table -- no improvement
				</section>

				<section>
					<h2>Is Deep Learning a myth?</h2>
				</section>

				<section>
					<h2>Modified intermediate layer</h2>
					Introduce non-linearity in the intermediate layer
				</section>

				<section>
					<h2>Non-linear activation functions</h2>
					<table>
						<tr>
							<td>
								<img src="images/latex_generated_images/sigmoid.png">
							</td>
							<td>
								<img src="images/latex_generated_images/tanh.png">
							</td>
							<td>
								<img src="images/latex_generated_images/relu.png">
							</td>
						</tr>
					</table>

				</section>

				<section>
					<h2>Results</h2>
					table -- show improvement
				</section>

				<section>
					<h2>What the hidden layer bought us</h2>

					what can we now model with the hidden layer?
					* some non-linearities, XOR. Concentric Circles??
				</section>

				<section>
					<h2>Back-propagation</h2>
					Briefly say that the chain rule in this context is called backprop,
					we don't really need to worry about it due to the beauty of autodiff
				</section>

				<section>
					<h2>Let's go deeper - add a layer</h2>
				</section>

				<section>
					<h2>Going deeper</h2>
					We're not doing deep learning yet. Graph of new layer
				</section>

				<section>
					<h2>Going deeper</h2>
					Code for adding a new layer. Just define new parameters and change the model.
				</section>

				<section>
					<h2>Results</h2>
					table -- show improvement, maybe in training error only?
					Or both training and testing but only have a couple extra layers
				</section>

				<section>
					<h2>What does going deep buy us?</h2>
					Learning higher-level features
				</section>

				<section>
					<h2>What does going deep buy us?</h2>
					Show features learned in convnet for images: dots, edges, etc ---> objects
				</section>

				<section>
					<h2>What does going deep buy us?</h2>
					Incidentally, this is how neural style transfer works (maybe include this?)
				</section>

				<section>
					<h2>What does going deep buy us?</h2>
					What about if we're doing text and passing in words? Get word embeddings
				</section>

				<section>
					<h2>What does going deep buy us?</h2>
					Talk about word2vec (show diagram), but say that it's not actually learned
					via deep learning. Can transform one-hot vectors to word2vec vectors,
					which more or less skips a layer
				</section>

				<section>
					<h2>What does going deep buy us?</h2>
					In general, very difficult to interpret. Active research.
				</section>

				<section>
					<h2>Regularization</h2>
				</section>

				<section>
					<h2>The dangers of overfitting</h2>
			    show that at some point our testing error goes up
				</section>

				<section>
					<h2>Regularization</h2>
					Explain concept of regularization: instead of letting
					training data take over, add some constraints on
					the weights that limits the dangers of overfitting
				</section>

				<section>
					<h2>L2 regularization</h2>
					Limiting the size of coefficients -- motivate why it's a good desideratum
				</section>

				<section>
					<h2>L2 regularization</h2>
					Show in code where L2 regularization should go,
					modify for TensorFlow implementation
				</section>

				<section>
					<h2>Dropout</h2>
					Another consideration -- reinforce redundancy
				</section>

				<section>
					<h2>Dropout</h2>
					Animation showing removal
				</section>

				<section>
					<h2>Implementing dropout in TensorFlow</h2>
					Show code for adding dropout layer
				</section>

				<section>
					<h2>Results</h2>
					Show that our training error goes up but testing error goes down
				</section>

<!-- end section on regularization -->
<!-- start conclusion -->


<!-- maybe review the steps and briefly say what else is possible?

model: changing # of hidden layers, # of neurons per hidden layer,
       some different architectures, RNNs, CNNs
cost function: some different possibilities?
optimizer: change up the alpha, AdaGrad, etc?

-->

				<section>
					<h2>Exploring more: architectures</h2>

					Show diagram from neural network zoo,
					maybe highlight CNNs and RNNs and briefly
					say that connecting them is exciting.
				</section>

				<section>
					<h2>Exploring more: Keras</h2>

					<!-- analogy -->
$$
\begin{align*}
\textrm{numpy} &: \textrm{scikit-learn} \\
&:: \\
\textrm{TensorFlow} &: \textrm{Keras}
\end{align*}
$$
				</section>

				<section>
					<h2>Exploring more: Keras</h2>

					<!-- translate our final diagram to Keras code -->
				</section>

				<section>
					<h2>Final thoughts</h2>

					<ul>
					  <li class="fragment">If you're familiar with traditional ML,<br>you can do deep learning!</li>
					  <li class="fragment">But try traditional ML first!</li>
						<li class="fragment">Go forth and experiment!</li>
				  </ul>
				</section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: true,
				transition: 'slide',

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 				  { src: 'plugin/math/math.js', async: true }
				]
			});
		</script>
	</body>
</html>
